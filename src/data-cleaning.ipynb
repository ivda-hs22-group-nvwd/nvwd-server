{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # DEBUG\n",
    "from glob import glob\n",
    "from pandas_profiling import ProfileReport\n",
    "import yaml\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ABS_SRC = os.getcwd()\n",
    "PATH_REL = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mapping File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"naming.yaml\") as stream:\n",
    "    naming = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/profiles_revised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title='Pandas Profilign Report')\n",
    "#profile.to_widgets() # does not show anything\n",
    "profile.to_notebook_iframe()\n",
    "profile.to_file(\"pandas_profiling_data_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_col_values(list, filename):\n",
    "    with open(r'{}.txt'.format(filename), 'w') as fp:\n",
    "        for element in list:\n",
    "            fp.write(\"{}\\n\".format(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zodiac Sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "zodiacs = df.sign.unique()\n",
    "print_col_values(list=zodiacs, filename='zodiacs')\n",
    "\n",
    "ZODIAC_STRING_REPLACMENT = '&rsquo;' # corresponds to \" ' \"\n",
    "\n",
    "# Clean\n",
    "zodiacs = [z for z in zodiacs if str(z) != 'nan'] # remove nan values\n",
    "zodiacs = [v.replace('&rsquo;', '\\'') for v in zodiacs] # replace '\n",
    "\n",
    "# Check\n",
    "print_col_values(list=zodiacs, filename='zodiacs-cleaned')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_zodiac = df.copy()\n",
    "\n",
    "# nan's, and spelling\n",
    "df_zodiac.dropna(inplace=True, subset=['sign']) # remove nan's\n",
    "df_zodiac.shape # (48890, 19) , same as profiler\n",
    "\n",
    "# extract only sign\n",
    "df_zodiac['sign-extracted'] = df_zodiac['sign'].str.split(' ').str[0]\n",
    "\n",
    "# extract sign modifier\n",
    "df_zodiac['sign-modifier-extracted'] = df_zodiac['sign'].str.split(' ').str[1:]\n",
    "df_zodiac['sign-modifier-extracted'] = df_zodiac['sign-modifier-extracted'].apply(lambda y: '' if len(y)==0 else y) # replace empty lists with ''\n",
    "df_zodiac['sign-modifier-extracted'] = df_zodiac['sign-modifier-extracted'].apply(lambda y: ' '.join(y) if len(y)!=0 else y) # join list of strings together\n",
    "df_zodiac['sign-modifier-extracted'] = df_zodiac['sign-modifier-extracted'].str.replace(ZODIAC_STRING_REPLACMENT,'\\'')  # replace \n",
    "\n",
    "# map sign modifier + ordinal classifier\n",
    "mapper_naming_dict = naming['zodiac_hierarchy'] \n",
    "df_zodiac['sign-modifier-extracted-ordinal'] =  df_zodiac['sign-modifier-extracted'].map(mapper_naming_dict).fillna(df_zodiac['sign-modifier-extracted']) # map values from dict according to string\n",
    "df_zodiac[['sign', 'sign-extracted', 'sign-modifier-extracted', 'sign-modifier-extracted-ordinal']]\n",
    "\n",
    "# encode signs\n",
    "sign_encoder = LabelEncoder()\n",
    "sign_encoder.fit(df_zodiac['sign-extracted'])\n",
    "encoded_col_sign = sign_encoder.transform(df_zodiac['sign-extracted'])\n",
    "df_zodiac['sign-extracted-categorical'] = encoded_col_sign\n",
    "df_zodiac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "lng = df.speaks.unique()\n",
    "print_col_values(list=lng, filename='languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_languages = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_languages.dropna(inplace=True, subset=['speaks']) # remove nan's\n",
    "df_languages.shape # (59896, 19) , same as profiler\n",
    "\n",
    "df_languages['spoken_languages'] = np.nan\n",
    "# structure: language (level), langauge2 .... || language, langauge2, ...\n",
    "# due to the n:m relationship between persons and languages we will choose a one key encoding for the data\n",
    "# to not overload the main dataframe we will create a separate df with the information of the languages and use teh same ID as in the main dataframe. The structure of the new df will look as followed:\n",
    "\n",
    "########################################################\n",
    "#  ID #  english  #  italian  #  spanish  # ... other # number of languages spoken\n",
    "#  1       1           0           0         0    1           2\n",
    "#  2       1           1           1         1    1           5\n",
    "#  3       0           1           1         0    1           3\n",
    "#  4       1           0           0         0    0           1\n",
    "#  5       1           0           1         0    1           3\n",
    "#\n",
    "# extract language, without level => split by comma => multiple values, split by space, use first\n",
    "#\n",
    "\n",
    "# iterate through all values and create lists for all languages, userIDs and column names\n",
    "allLanguages = list()\n",
    "allIds = list()\n",
    "languageColumns = list()\n",
    "for row in df_languages.iterrows():\n",
    "    languages = list()\n",
    "    idsForPerson = list()\n",
    "    # in this step the language is extracted (the appendix (spoken level e.g. fluid) removed)\n",
    "    for language in row[1].speaks.split(','):\n",
    "        languages.append(language.strip().split(' ')[0])\n",
    "        # indexes are going to be added to the list for the next step (one hot key encoding)\n",
    "        idsForPerson.append(row[0])\n",
    "        languageColumns.append(language.strip().split(' ')[0])\n",
    "    # allLanguages represents a list of languages [english, french, spanish]\n",
    "    allLanguages.append(languages)\n",
    "    # allIds represents a the list of the corresponding index [6, 6, 6] (for the lambda below)\n",
    "    allIds.append(idsForPerson)\n",
    "\n",
    "# remove all the duplicates from the column list\n",
    "languageColumns = sorted(set(languageColumns))\n",
    "encoded_df_languages = pd.DataFrame()\n",
    "df_languages['language'] = allLanguages\n",
    "encoded_df_languages['language'] = allLanguages\n",
    "encoded_df_languages['userID'] = allIds\n",
    "\n",
    "# split up the values from the array into new rows\n",
    "# [english, french, spanish] will be 3 rows with the corresponding userID of 6, 6, 6 => df gets form 60k rows to 110k\n",
    "encoded_df_languages =encoded_df_languages.apply(lambda x: pd.Series(np.concatenate(x.tolist())), 0)\n",
    "\n",
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#perform one-hot encoding on 'language' column\n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(encoded_df_languages[['language']]).toarray())\n",
    "encoder_df.columns = languageColumns\n",
    "# join the new encoded df with the language one\n",
    "encoded_df_languages = encoded_df_languages.join(encoder_df)\n",
    "encoded_df_languages.drop(['language'], axis=1, inplace=True)\n",
    "# group the user that the languages are shown in one row => back to 60k rows\n",
    "encoded_df_languages.groupby('userID').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Body Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "body_type = df['body_type'].unique()\n",
    "print_col_values(list=body_type, filename='body_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_bodyt = df.copy()\n",
    "\n",
    "# nan's, and spelling\n",
    "df_bodyt.dropna(inplace=True, subset=['sign']) # remove nan's\n",
    "df_bodyt.shape # (48890, 19) , same as profiler\n",
    "\n",
    "# map sign modifier + ordinal classifier\n",
    "mapper_body_type_dict = naming['body_type'] \n",
    "df_bodyt['body_type_ordinal'] =  df_bodyt['body_type'].map(mapper_body_type_dict).fillna(df_bodyt['body_type']) # map values from dict according to string\n",
    "df_bodyt.dropna(inplace=True, subset=['body_type_ordinal']) # todo better solution?\n",
    "df_bodyt\n",
    "\n",
    "# todo Consultation whether mapping (clearly unhealthy => -1, not optimal/unknown => 0, else => +1) justifiable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "diet = df.diet.unique()\n",
    "print_col_values(list=diet, filename='diet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_diet = df.copy()\n",
    "\n",
    "# nan's, and spelling\n",
    "df_diet.dropna(inplace=True, subset=['diet']) # remove nan's\n",
    "df_diet.shape # (48890, 19) , same as profiler\n",
    "\n",
    "# extract only sign\n",
    "df_diet['diet_extracted'] = df_diet['diet'].str.split(' ').str[-1]\n",
    "\n",
    "# extract sign modifier\n",
    "df_diet['diet_modifier_extracted'] = df_diet['diet'].str.split(' ').str[:-1]\n",
    "df_diet['diet_modifier_extracted'] = df_diet['diet_modifier_extracted'].apply(lambda y: '' if len(y)==0 else y[0]) # replace empty lists with '' and extract term from list\n",
    "df_diet\n",
    "# todo Consultation whether further mapping makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "drugs = df.drugs.unique()\n",
    "print_col_values(list=drugs, filename='drugs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_drugs = df.copy()\n",
    "\n",
    "# nan's, and spelling\n",
    "df_drugs.dropna(inplace=True, subset=['drugs']) # remove nan's\n",
    "df_drugs.shape # (45866, 19) , same as profiler\n",
    "\n",
    "# map ordinal classifier\n",
    "mapper_drugs_dict = naming['drugs']\n",
    "df_drugs['drugs_ordinal'] =  df_drugs['drugs'].map(mapper_drugs_dict).fillna(df_drugs['drugs']) # map values from dict according to string\n",
    "df_drugs['drugs_ordinal'] = df_drugs['drugs_ordinal'].apply(lambda y: y if (y in [-2, -1, 1]) else 0) # map empty to 0\n",
    "\n",
    "# todo Consultation whether mapping (clearly unhealthy => -2, not optimal => -1, optimal => +1, unknown => 0) justifiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Drinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "drinks = df.drinks.unique()\n",
    "print_col_values(list=drinks, filename='drinks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_drinks = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_drinks.dropna(inplace=True, subset=['drinks']) # remove nan's\n",
    "df_drinks.shape # (56961, 19) , same as profiler\n",
    "\n",
    "# map ordinal classifier\n",
    "mapper_drinks_dict = naming['drinks']\n",
    "df_drinks['drinks_ordinal'] =  df_drinks['drinks'].map(mapper_drinks_dict).fillna(df_drinks['drinks'])\n",
    "# df_drinks['drinks_ordinal'] = df_drinks['drinks_ordinal'].apply(lambda y: y if (y in [-2, -1, 0, 1]) else 0) # todo: check if needed # map empty to 0\n",
    "# todo Consultation whether mapping (clearly unhealthy => -2, \"social drinker\" => 0, optimal => +1) justifiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "education = df.education.unique()\n",
    "print_col_values(list=education, filename='education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE BEFORE PUSH\n",
    "with open(\"naming.yaml\") as stream:\n",
    "    naming = yaml.safe_load(stream)\n",
    "\n",
    "# copy by value\n",
    "df_education = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_education.dropna(inplace=True, subset=['education']) # remove nan's\n",
    "df_education.shape # (53318, 19) , same as profiler\n",
    "\n",
    "# extract only education institution and status \n",
    "# todo find better solution to use the dedicated mapper in naming.yaml\n",
    "def educationa_status_mapper(x):\n",
    "    if 'dropped out of' in x:\n",
    "        return 'dropped out of'\n",
    "    if 'working on' in x:\n",
    "        return 'working on'\n",
    "    if 'graduated from' in x:\n",
    "        return 'graduated from'\n",
    "\n",
    "def educationa_institution_mapper(x):\n",
    "    if 'college/university' in x:\n",
    "        return 'college/university'\n",
    "    if 'two-year college' in x:\n",
    "        return 'two-year college'\n",
    "    if 'masters program' in x:\n",
    "        return 'masters program'\n",
    "    if 'ph.d program' in x:\n",
    "        return 'ph.d program'\n",
    "    if 'high school' in x:\n",
    "        return 'high school'\n",
    "    if 'law school' in x:\n",
    "        return 'law school'\n",
    "    if 'med school' in x:\n",
    "        return 'med school'\n",
    "    if 'space camp' in x:\n",
    "        return 'space camp'\n",
    "\n",
    "df_education['education_status_extracted'] = df_education['education'].apply(lambda x: educationa_status_mapper(x))\n",
    "df_education['education_instituation_extracted'] = df_education['education'].apply(lambda x: educationa_institution_mapper(x))\n",
    "\n",
    "\n",
    "# map ordinal classifier\n",
    "mapper_education_status_hierarchy_dict = naming['education_status_hierarchy']\n",
    "df_education['education_status_ordinal'] =  df_education['education_status_extracted'].map(mapper_education_status_hierarchy_dict).fillna(df_education['education_status_extracted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "ethnicity = df.ethnicity.unique()\n",
    "print_col_values(list=ethnicity, filename='ethnicity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "income = df.income.unique()\n",
    "print_col_values(list=income, filename='income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_income = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_income.dropna(inplace=True, subset=['income']) # remove nan's\n",
    "df_income = df_income[df_income.income != -1] # do not consider the -1 values (nan values)\n",
    "df_income.shape # (11504, 19) , MAX REDUCTION!\n",
    "\n",
    "df_income['income'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "age = df.age.unique()\n",
    "print_col_values(list=age, filename='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_age = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_age.dropna(inplace=True, subset=['age']) # remove nan's\n",
    "df_age.shape # (11504, 19) , MAX REDUCTION!\n",
    "\n",
    "#df_age['age'].value_counts().sort_index().plot(kind='bar', figsize=(12,4))\n",
    "df_age['age'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "job = df.job.unique()\n",
    "print_col_values(list=job, filename='job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "offspring = df.offspring.unique()\n",
    "print_col_values(list=offspring, filename='offspring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "orientation = df.orientation.unique()\n",
    "print_col_values(list=orientation, filename='orientation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "pets = df.pets.unique()\n",
    "print_col_values(list=pets, filename='pets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Religion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "religion = df.religion.unique()\n",
    "print_col_values(list=religion, filename='religion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Smokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "smokes = df.smokes.unique()\n",
    "print_col_values(list=smokes, filename='smokes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy by value\n",
    "df_smokes = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_smokes.dropna(inplace=True, subset=['smokes']) # remove nan's\n",
    "df_smokes.shape # (54434, 19) , same as profiler\n",
    "\n",
    "smokes_encoder = category_encoders.OrdinalEncoder(\n",
    "    cols = ['smokes'],\n",
    "    return_df = True,\n",
    "    mapping = [naming['smokes']]\n",
    ")\n",
    "\n",
    "df_smokes['smokes_ordinal'] = smokes_encoder.fit_transform(df_smokes['smokes'])\n",
    "df_smokes[['smokes_ordinal', 'smokes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "sex = df.sex.unique()\n",
    "print_col_values(list=sex, filename='sex')\n",
    "# copy by value\n",
    "df_sex = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_sex.dropna(inplace=True, subset=['sex']) # remove nan's\n",
    "df_sex.shape # (59946, 19) , same as profiler\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "df_sex['sex_categorical'] = sex_encoder.fit_transform(df_sex['sex'])\n",
    "df_sex[['sex_categorical', 'sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Col\n",
    "status = df.status.unique()\n",
    "print_col_values(list=status, filename='status')\n",
    "# copy by value\n",
    "df_status = df.copy()\n",
    "\n",
    "# nan's\n",
    "df_status.dropna(inplace=True, subset=['status']) # remove nan's\n",
    "df_status.shape # (59946, 19) , same as profiler\n",
    "\n",
    "status_encoder = LabelEncoder()\n",
    "df_status['status_categorical'] = status_encoder.fit_transform(df_status['status'])\n",
    "df_status[['status_categorical', 'status']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
